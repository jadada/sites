---
layout: cover
coverDate: ""
---

# 3. AlphaZero (2018)

<br>

<img src='/[3.AlphaZero]-01.title.png' class='shadow-lg mx-auto w-160'>


<p class='text-sm'>

[Homepage](https://www.deepmind.com/open-source/alphazero-resources) & [Online PDF](https://mnomqrns7c.feishu.cn/file/boxcnwFu97mp1IVNBtqzD4EMsUe)

</p>

---

# How does it extend AlphaGo Zero?

A more generic version of the AlphaGo Zero that accommodates, without special casing, a broader class of game rules.

- Use the same algorithm, network architecture and hyperparameters for chess and shogi, as well as Go.

- Go games have a binary win or loss outcome, whereas both chess and shogi may end in drawn outcomes. The game outcome is <font class='text-red-600'> $z=\{0,\pm1\}$ </font>: <u>-1 for loss, 0 for draw, 1 for win</u>.

- To accommodate a broader class of games, AlphaZero <u>does not assume symmetry</u>. AlphaZero does not augment the training data and does not transform the board position during MCTS.

- AlphaZero always generate self-play games <u>with the newest player</u> of last iteration, instead of the best player from all previous iterations as in AlphaGo Zero.


---

# State Representation

feature size: $N \times N \times (MT + L)$:

- $N \times N$ represents the board size.

- $T$ represents the board positions history at time steps $[t-T+1, \dots, t]$.

- $M$ represents the presence of player's pieces.

- $L$ denotes the player's color, the move number and other states of special rules.

- Illegal moves are <font class='text-red-600'>masked out</font> by <u>setting their probabilities to zero</u>, and <u>re-normalising</u> the probabilities over the remaining set of legal moves.


<div class='group'>
    <a href='#' class='top-10 right-20 absolute group-hover:font-semibold'>Representation</a>
    <img src='/[3.AlphaZero]-02.feature_representation.png' class='h-90 invisible group-hover:visible top-15 right-18 absolute border-2'>
</div>


---
layout: two-cols 
---

# Method overwiew

<br>

The self-play training process and the neural network architecture (1 Conv block + 19 Res block) are basically <u>the same as AlphaGo Zero</u>.

$$z_t = \pm1, 0$$

$$(\mathbf{p}, v) = f_\theta(s)$$

$$l = (z-v)^2 - \mathbf{\pi}^T\log \mathbf{p} + c||\theta||^2$$


::right::


<img src='/[2.AlphaGo_Zero]-02-0.self-play.png' class='h-100 mx-auto'>


<p class='text-xs text-center'>

Self-play training process in AlphaGo Zero.

</p>


---

# Results

### Performance

<br>

<img src='/[3.AlphaZero]-03.performance.png' class='h-60 mx-auto'>

<div class='text-sm'>

AlphaZero used a single machine with 4 TPUs (same as AlphaGo Zero). Milestones:
- chess: 4 hours (300,000 steps) against Stockfish;
- shogi: 2 hours (110,000 steps) against Elmo;
- Go: 30 hours (74,000 steps) against AlphaGo Lee.

</div>


---

# Results

### Comparison with specialized programs

<br>

<img src='/[3.AlphaZero]-04.programs_comparison.jpg' class='h-90 mx-auto'>

