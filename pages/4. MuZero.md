---
layout: cover
coverDate: ""
---

# 4. MuZero (2020)

<br>

<img src='/[4.MuZero]-01.title.png' class='shadow-lg mx-auto w-220'>


<p class='text-sm'>

[Homepage](https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules) & [Online PDF](https://mnomqrns7c.feishu.cn/file/boxcnMst8DYnhuH6fb3t1LXNBpb)

</p>

---

# What's different?

MuZero extends AlphaZero to a broader set of environments (e.g. Atari2600), including <u>single agent</u> domains and <u>non-zero rewards</u> at intermediate time steps.

No rules were given in advance, MuZero learned the rules by its internal networks.

- <font class='text-red-600 font-bold'>State transitions</font>: AlphaZero had access to a perfect simulator of the environmentâ€™s state-to-state dynamics. In contrast, MuZero employs <u>a learned dynamics model</u> to estimate the simulator.

- <font class='text-red-600 font-bold'>Actions available</font>: AlphaZero used the set of legal actions obtained from the simulator to mask the policy network at interior nodes. MuZero <u>does not perform any masking</u> within the search tree.

- <font class='text-red-600 font-bold'>Terminal states</font>: AlphaZero stopped at the terminal states directly provided by the simulator. MuZero does not give special treatment to terminal states and always <u>uses the value predicted</u> by the network.


---

# Training process

<img src='/[4.MuZero]-02-0.training_process.jpg' class='h-90 mx-auto'>

<p class='text-xs text-center'>
Overall training process of MuZero.
</p>


---

## Part 1: MCTS search

<br>

- Aside from $(Q, N, P)$ as before, the edges in MuZero MCTS also store <font class='text-red-600'>intermediate reward</font> $R(s,a)$ & <font class='text-red-600'>state transition</font> $S(s,a)$.

- The evaluating value is the $k$-step <u>cumulative discounted reward</u>:

$$V(s_t) = \sum_{\tau=0}^{k-1} \gamma^\tau r_{t+\tau} + \gamma^k v_{t+k}$$

- Because the value above is unbounded, the action-state value $Q$ would be <u>normalized</u>:

$$Q(s,a) = \frac{1}{N(s,a)} \sum_{i=1}^N 1(s,a,i) V(s_L^i)$$
$$\bar{Q}(s,a) = \frac{Q(s,a) - \min Q}{\max Q - \min Q}$$


<div class='group'>
    <a href='#' class='top-10 right-20 absolute group-hover:font-semibold'>MCTS</a>
    <img src='/[4.MuZero]-02-1.MCTS.jpg' class='h-90 invisible group-hover:visible top-15 right-18 absolute border-2'>
</div>


---

## Part 2: Self-play process

<br>

<img src='/[4.MuZero]-02-2.self-play.jpg' class='h-50 mx-auto'>

- Self-play process is basically the same, except <font class='text-red-600'>intermediate rewards</font> $u_t$ are considered.

- The trajectories of self-play would be stored into a <font class='text-red-600 font-bold'>replay buffer</font> (off-policy RL).

- The final outcome $z_t$ is computed as the <u>cumulative discounted reward</u>:

$$z_t = u_{t+1} + \gamma u_{t+2} + \dots + \gamma^{n-1}u_{t+n} + \gamma^n v_{t+n}$$


---

## Part 3: Network training

<br>

A trajectory is sampled from the <u>replay buffer</u>. 
1. The <font class='text-red-600'>representation</font> function $h$ generate <u>the first hidden state</u> representation $s_0$.
2. The <font class='text-red-600'>dynamics</font> function $g$ recursively predicts <u>subsequent rewards</u> $r_t$ and <u>states</u> $s_t$ based on the predicted state $s_{t-1}$ and the real action $a_t$. 
3. The <font class='text-red-600'>prediction</font> function $f$ predicts the <u>distribution</u> $p_t$ and <u>value</u> $v_t$ for these predicted states.
4. Train the model by minimizing the loss.

$$l = (z-v)^2 - \mathbf{\pi}^t\log \mathbf{p} + (u-r)^2+c||\theta||^2$$


The network architecture is basically the same as in AlphaZero, but <u>with 16 instead of 20</u> residual blocks in <font class='text-red-600'>representation</font> and <font class='text-red-600'>dynamics</font> networks.


<div class='group'>
    <a href='#' class='top-10 right-20 absolute group-hover:font-semibold'>training process</a>
    <img src='/[4.MuZero]-02-3.network_training.jpg' class='h-60 invisible group-hover:visible top-15 right-18 absolute border-2'>
</div>


---

# Reanalyze

MuZero Reanalyze revisits its <u>past time steps</u> and re-executes MCTS search <u>using the latest model parameters</u>, potentially resulting in a better-quality policy than the original search.

- This fresh policy is used as the policy target for <font class='text-red-600'>80% of updates</font> during MuZero training. 

<br>

<div class='grid grid-cols-2'>
    <div>
        <img src='/[4.MuZero]-03-1.normal_training_loop.jpg' class='h-60 mx-auto'>
        <p class='text-xs text-center'>Normal training loop.</p>      
    </div>
    <div>
        <img src='/[4.MuZero]-03-2.reanalyze_training_loop.jpg' class='h-70 mx-auto'>
        <p class='text-xs text-center'>Reanalyze training loop.</p>    
    </div>
</div>


---

# Results

<br>

<img src='/[4.MuZero]-04.performance_against_AlphaZero_and_R2D2.png' class='h-100 mx-auto'>

---

# Results

<br>

<img src='/[4.MuZero]-05.evaluations.png' class='h-100 mx-auto'>

---

# Results

<br>

<img src='/[4.MuZero]-06.Reanalyze_performance.png' class='h-70 mx-auto'>

