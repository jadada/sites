---
layout: cover
coverDate: ""
---

# 2. AlphaGo Zero (2017)

<br>

<img src='/[2.AlphaGo_Zero]-01.title.png' class='shadow-lg mx-auto w-220'>


<p class='text-sm'>

[Homepage](https://www.deepmind.com/blog/alphago-zero-starting-from-scratch) & [Online PDF](https://mnomqrns7c.feishu.cn/file/boxcnRiLn3v6dKkklJ8yzO8uinc)

</p>

---

# How does it differ from AlphaGo?

1. AlphaGo Zero is <font class='text-red-600'>trained solely by self-play</font>, without any supervision or use of human data.

2. AlphaGo Zero <font class='text-red-600'>only uses the black and white stones</font> from the Go board as its input.

3. AlphaGo Zero uses a <font class='text-red-600'>single neural network</font>, rather than separate policy and value networks.

4. AlphaGo Zero <font class='text-red-600'>does not use “rollouts”</font> to evaluate positions and sample moves. Instead, it uses a simpler tree search that relies upon this single neural network.


---
layout: two-cols
---

# Self-play training pipeline

<div class='text-sm'>

- A game terminates <font class='text-red-600'>when both players pass</font>, when the search value drops <u>below a resignation threshold</u> or when the game <u>exceeds a maximum length</u>.

- After each iteration of training, the performance of the new player was measured against the best player; if the new player <u>won by a margin of 55%</u>, then it replaced the best player

$$z_t = \pm1$$
$$(\mathbf{p}, v) = f_\theta(s)$$
$$l = (z-v)^2 - \mathbf{\pi}^T\log \mathbf{p} + c||\theta||^2$$

</div>

::right::

<br>

<img src='/[2.AlphaGo_Zero]-02-0.self-play.png' class='mx-auto h-90'>

<p class='text-xs text-center'>
    Self-training pipeline
</p>

---

# MCTS in AlphaGo Zero

Basically the same as AlphaGo. Sightly different in <font class='text-red-600 font-bold'>evaluate</font> and <font class='text-red-600 font-bold'>play</font> phase.

- <font class='text-red-600 font-bold'>Evaluate</font>: <u>Without random rollouts</u> as in AlphaGo, the action value $V(s)$ of the node $s$ are fully evaluated by the neural network.
$$(P(s,\cdot), V(s)) = f_\theta(s)$$

- <font class='text-red-600 font-bold'>Play</font>: Once the search is complete, search probabilities $\pi$ are returned, proportional to $N^{1/\tau}$.
    - $N$: the <u>visit count</u> of each move from the root state.
    - $\tau$: temperature param that controls the level of exploration. ($\tau=0$: <u>greedy search</u>; $\tau \rightarrow \infty$: <u>uniform sample</u>)

$$\pi(a|s_0) = \frac{N(s_0,a)^{1/\tau}}{\sum_b N(s_0,b)^{1/\tau}}$$

- the child node corresponding to the played action <u>becomes the new root node</u>; retain the subtree below this child, <u>discard the remainder</u>.


<div class='group'>
    <a href='#' class='top-10 right-20 absolute group-hover:font-semibold'>MTCS</a>
    <img src='/[2.AlphaGo_Zero]-03.MCTS.png' class='h-60 invisible group-hover:visible top-15 right-18 absolute border-2'>
</div>


---

# Network Architecture

- 1 <font class='text-red-600 font-bold'>convolutional</font> block followed by 19 or 39 <font class='text-red-600 font-bold'>residual</font> block.
    - <font class='text-red-600'>Conv block</font>: conv $\rightarrow$ BN $\rightarrow$ relu
    - <font class='text-red-600'>Res block</font>: conv $\rightarrow$ BN $\rightarrow$ relu $\rightarrow$ conv $\rightarrow$ BN $\rightarrow$ skip-connect $\rightarrow$ relu

- Two output heads for computing the <u>policy</u> and <u>value</u>.

- Feature size: $19 \times 19 \times 17$ (8 stone history x 2 player existence + 1 color), <u>no hand-crafted features</u> as in AlphaGo.

$$s_t = [X_t, Y_t, X_{t-1}, Y_{t-1}, \dots, X_{t-7}, Y_{t-7}, C]$$

- Similar to AlphaGo, AlphaGo Zero also augments the training dataset by <u>sampling random rotations or reflections</u> of the positions during MCTS.

---

# Results

### Performance of AlphaGo Zero

<br>

<img src='/[2.AlphaGo_Zero]-04.performance.png' class='h-75 mx-auto'>

<p class='text-xs text-center'>
AlphaGo Zero used a single machine with 4 tensor processing units (TPUs), whereas AlphaGo Lee was distributed over many machines and used 48 TPUs. AlphaGo Zero defeated AlphaGo Lee <u>by 100 games to 0</u>.
</p>


---

# Results

### Network Architecture comparison

<br>

<img src='/[2.AlphaGo_Zero]-05.architecture_comparison.png' class='h-75 mx-auto'>

<br>

<div class='text-sm'>

- <font class='text-red-600'>sep</font> / <font class='text-red-600'>dual</font>: seperate/combined policy and value networks.
- <font class='text-red-600'>dual-res</font> and <font class='text-red-600'>sep-conv</font> correspond to the networks used in AlphaGo Zero and AlphaGo Lee, respectively.

</div>


---
layout: two-cols
---

# Results

### Go knowledge learned by AlphaGo Zero

<br>



<div class='text-sm'>

- **a**: Five human *joseki* (common corner sequences) discovered during AlphaGo Zero training.

- **b**: Five *joseki* favoured at different stages of self-­play training. 

- **c**: The first 80 moves of three self­play games that were played at different stages of training.

</div>

::right::

<p class='break'></p>

<img src='/[2.AlphaGo_Zero]-06.learned_knowledge.png' class='h-100 mx-auto'>


