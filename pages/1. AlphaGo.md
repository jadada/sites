---
layout: cover
coverDate: ""
---

# 1. AlphaGo (2016)

<br>

<img src='/[1.AlphaGo]-01.title.png' class='shadow-lg mx-auto w-220'>

<p class='text-sm'>

[Homepage](https://www.deepmind.com/publications/mastering-the-game-of-go-with-deep-neural-networks-tree-search) & [Online PDF](https://mnomqrns7c.feishu.cn/file/boxcnUhiHrgvGlFlraNwpoJMCVq)

</p>

---
layout: two-cols
---
# Tree search in Go

<br>

- Go is a zero-sum game played turn-by-turn on a 19x19 board. The searching process in Go could be perfectly described as a <font class='text-red-600 font-bold'>tree search</font> algorithm.
    <!-- - Nodes represent states; Edges represent actions go from one state to another. 
    - The goal is to find the best path to follow in order to win the game.  -->

- There are theoretically 361! possible situations in a single game. The key to the problem is to reduce the complexity (e.g. Alpha-Beta Pruning, MCTS).

::right::

<br><br>

<img src='/[1.AlphaGo]-02.Tree_search_in_go.jpg' class='mx-auto h-75'>


---

# Training pipeline in AlphaGo

<img src='/[1.AlphaGo]-03-0.Overall_training_process.png' class='h-90 mx-auto'>

<p class='text-sm text-center'>
    Two-stage training pipeline
</p>


---
layout: two-cols
---

## Phase 1: Supervised Learning

<br>

- Train 2 policy networks: <font class='text-red-600'>Rollout policy</font> & <font class='text-red-600'>SL policy network</font>. 

- Learn from <u>human expert positions data</u> to predict the distribution of human expert moves (Classification).

- <font class='text-red-600 font-bold'>SL policy network</font>: larger, more accurate but slower. Used in RL stage.

- <font class='text-red-600 font-bold'>Rollout policy</font>: smaller, faster but less accurate. Using smaller pattern features. Used in MCTS.


::right::

<br>

<img src='/[1.AlphaGo]-03-1.SL_training_process.png' class='mx-auto h-95'>

<p class='text-xs text-center'>
    Supervised learning process
</p>


---
layout: two-cols
---

## Phase 2: Reinforcement Learning

<br>

Using **Policy Iteration**:

1. Policy improvement (<font class='text-red-600'>Policy network</font>);

2. Policy evaluation (<font class='text-red-600'>Value network</font>).


::right::

<br>

<img src='/[1.AlphaGo]-03-2.RL_training_process.png' class='mx-auto h-95'>

<p class='text-xs text-center'>
    Reinforcement learning process
</p>

---
layout: two-cols
---

## Phase 2: Reinforcement Learning

<br>

<font class='text-red-600 font-bold'>Policy network</font>: 

- initialized as the <font class='text-red-600'>SL policy network</font>. The output is the action distribution according to the board state. 

- **Self-play**: play games between current policy network and a <u>randomly selected previous iteration</u> of the policy network. Stabilize training by preventing overfitting to the current policy.

- Every 500 iterations, add the current parameters $\rho$ to the opponent pool.


::right::

<br>

<img src='/[1.AlphaGo]-04.model_architecture.png' class='mx-auto h-95'>

<p class='text-xs text-center'>
    Network Architecture
</p>


---
layout: two-cols
---

## Phase 2: Reinforcement Learning

<br>

<font class='text-red-600 font-bold'>Value network</font>:

<p class='break'></p>

- estimate a <u>value function</u> $v^p(s)$ that predicts the outcome $z=\pm1$ from positions of games played by using policy $p$ for both players (Regression).

$$v^p = \mathbb{E}[z_t |s_t=s, a_{t\dots T}\sim p]$$

- $z_t=\pm1$ is the <u>terminal reward</u> at the end of the game from the perspective of the current player at time step $t$. <u>Estimated by the MCTS</u>.


::right::

<br>

<img src='/[1.AlphaGo]-04.model_architecture.png' class='mx-auto h-95'>

<p class='text-xs text-center'>
    Network Architecture
</p>


---

# Neural Network Architecture

- 13-layer CNN 
- input feature/state size: 19x19x48
- The rules of Go are <u>invariant under rotation and reflection</u>. AlphaGo exploits this symmetrics by passing a mini-batch of all 8 positions into the policy network or value network and computed
in parallel.

<br>

<img src="/[1.AlphaGo]-05.input_feature.png" class="border h-65 mx-auto">


---

# Monte Carlo tree search (<font class='text-red-600'>MCTS</font>) in AlphaGo

<br>

- <font class='text-red-600 font-bold'>Monte Carlo method</font>: uses random sampling to estimate the expected outcomes for deterministic problems.

- MCTS is used to simulate potential actions to the best results without traversing every node in the search tree.

<br>

<img src="/[1.AlphaGo]-06-0.MCTS.png" class="border h-60 mx-auto">

---
layout: two-cols
---

## Step 1: Selection

<br>

Each simulation starts from the root node to the <u>leaf node</u> (unvisited, non-terminal node).

$$a_t = \argmax_a (Q(s_t,a) + u(s_t, a))$$

$$u(s,a) \propto \frac{P(s,a)}{1+N(s,a)}$$


::right::

<br>

<img src='/[1.AlphaGo]-06-1.MCTS_selection.png' class='mx-auto h-95'>

<p class='text-xs text-center'>
    MCTS selection
</p>


---
layout: two-cols
---

## Step 2: Expansion

<br>

When visiting to a leaf node, add new node to the search tree. 

Store the prior probability of the node.


::right::

<br>

<img src='/[1.AlphaGo]-06-2.MCTS_expansion.png' class='mx-auto h-95'>

<p class='text-xs text-center'>
    MCTS expansion
</p>


---
layout: two-cols
---

## Step 3: Evaluation

<br>

The leaf node is evaluated in two ways:

1. by the <font class='text-red-600'>Value network</font> $v_\theta(s_L)$

2. by the outcome $z_L=\pm1$ of a random rollout until terminal step using the <font class='text-red-600'>fast rollout policy</font> $p_\pi$.

<br>

$$V(s_L) = (1-\lambda)v_\theta (s_L) + \lambda z_L$$


::right::

<br>

<img src='/[1.AlphaGo]-06-3.MCTS_evaluation.png' class='mx-auto h-95'>

<p class='text-xs text-center'>
    MCTS evaluation
</p>


---
layout: two-cols
---

## Step 4: Backup

At the end of simulation, the action values and visit counts of all traversed edges are updated.

$$N(s,a) = \sum_{i=1}^N 1(s,a,i)$$

$$Q(s,a) = \frac{1}{N(s,a)} \sum_{i=1}^N 1(s,a,i) V(s_L^i)$$


::right::

<br>

<img src='/[1.AlphaGo]-06-4.MCTS_backup.png' class='mx-auto h-95'>

<p class='text-xs text-center'>
    MCTS backup
</p>


---

# Training target


- <font class='text-red-600 font-bold'>Rollout policy network</font> & <font class='text-red-600 font-bold'>SL policy network</font>: maximize the likelihood of the human move $a$ selected in state $s$.

$$\Delta \sigma \propto \frac{\partial \log P_{\sigma}(a|s)}{\partial \sigma}$$

- <font class='text-red-600 font-bold'>RL policy network</font>: maximize the expected outcome of the selected move.

$$\Delta \rho \propto \frac{\partial \log P_{\rho}(a|s)}{\partial \rho} z_t$$

- <font class='text-red-600 font-bold'>Value network</font>: minimize the mean squared error (MSE) between the predicted value $v_\theta(s)$, and the corresponding outcome $z$.

$$\Delta \theta \propto \frac{\partial v_\theta(s)}{\partial \theta} (z - v_\theta(s))$$



<div class='group'>
    <a href='#' class='top-10 right-15 absolute group-hover:font-semibold'>networks</a>
    <img src='/[1.AlphaGo]-03-0.Overall_training_process.png' class='h-70 invisible group-hover:visible top-15 right-18 absolute border-2'>
</div>


---

# Results

<br>

<img src='/[1.AlphaGo]-07.results.png' class='h-80 mx-auto'>

<p class='text-xs text-center'>
AlphaGo won <b>Fan Hui</b>, the winner of 2013, 2014 and 2015
European Go championships, <u>by 5:0</u> in a formal five-game match over 5-9 October 2015.
</p>


---

# Computing Expenses

- **Rollout policy** (MCTS): approximately 1,000 simulations per second per CPU thread on an empty board.
- **SL policy classification**: around 3 weeks for 340 million training steps, using 50 GPUs. 
- **RL policy network**: trained for 10,000 mini-batches of 128 games, using 50 GPUs, for one day.
- **Value network**: trained for 50 million mini-batches of 32 positions, using 50 GPUs, for one week.

<br>

<img src='/[1.AlphaGo]-08.computing_expenses.png' class='border h-65 mx-auto'>

